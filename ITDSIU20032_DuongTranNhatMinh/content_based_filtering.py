# -*- coding: utf-8 -*-
"""Content_Based_Filtering.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vIwyNeYCk91jZPUf57vIeQKaS9ucxsmz
"""

import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, precision_score, recall_score, f1_score
import numpy as np

# Preprocessing function with weights
def preprocess_data_with_weights(icat, category_weight=2, quality_weight=1):
    icat.fillna({'Category': '', 'Quality': icat['Quality'].mean()}, inplace=True)

    icat['Category'] = icat['Category'].str.strip("[]").str.replace("\'", "").str.split(", ")
    icat['Category'] = icat['Category'].apply(lambda x: " ".join(x))

    vectorizer = CountVectorizer()
    category_matrix = vectorizer.fit_transform(icat['Category'])

    scaler = MinMaxScaler()
    icat['Quality'] = scaler.fit_transform(icat[['Quality']])
    category_features = pd.DataFrame(category_matrix.toarray(), columns=vectorizer.get_feature_names_out())
    category_features = category_features * category_weight
    category_features['Quality'] = icat['Quality'] * quality_weight

    return icat, category_features

# Optimized evaluation function with additional metrics
def evaluate_model_weighted_optimized(icat, category_features, rats, threshold=0.5):
    # Fix: Merge on 'place_id' in both dataframes
    rats = rats.merge(icat, left_on='place_id', right_on='place_id')

    user_profiles = []
    for user_id in rats['user_id'].unique():
        user_ratings = rats[rats['user_id'] == user_id]
        # Fix: Use 'place_id' to access features
        user_profile = user_ratings[['rating']].T @ category_features.loc[user_ratings['place_id']].values
        user_profile = pd.DataFrame(user_profile).fillna(0)
        user_profiles.append(user_profile / user_profile.sum().sum())

    user_profiles = np.vstack(user_profiles)
    similarities = cosine_similarity(user_profiles, category_features.values)

    true_ratings = rats['rating'].values
    predicted_ratings = []

    for user_id, sim in zip(rats['user_id'].unique(), similarities):
        user_ratings = rats[rats['user_id'] == user_id]
        # Fix: Use 'place_id' to access similarities
        predicted_ratings.extend([sim[item] for item in user_ratings['place_id']])

    # RMSE Calculation
    rmse = np.sqrt(mean_squared_error(true_ratings, predicted_ratings))

    # Accuracy Calculation (normalized to max rating)
    accuracy = (1 - rmse / max(true_ratings)) * 100

    # Binary conversion for Precision, Recall, F1 Score
    predicted_binary = [1 if p > threshold else 0 for p in predicted_ratings]
    true_binary = [1 if r > threshold else 0 for r in true_ratings]

    # Precision, Recall, F1-Score Calculation
    precision = precision_score(true_binary, predicted_binary)
    recall = recall_score(true_binary, predicted_binary)
    f1 = f1_score(true_binary, predicted_binary)

    print(f"RMSE: {rmse:.4f}")
    print(f"Accuracy: {accuracy:.2f}%")
    print(f"Precision: {precision:.4f}")
    print(f"Recall: {recall:.4f}")
    print(f"F1-Score: {f1:.4f}")

    return accuracy, rmse, precision, recall, f1

# Load datasets
icat = pd.read_csv('https://raw.githubusercontent.com/NhatMinh2910/Pre-thesis-Datasets/refs/heads/main/icat.csv')
rats = pd.read_csv('https://raw.githubusercontent.com/NhatMinh2910/Pre-thesis-Datasets/refs/heads/main/rats.csv')

# Preprocess with weights
icat_processed, category_features_weighted = preprocess_data_with_weights(icat.copy()) # use a copy to avoid modifying the original icat

# Evaluate model with optimized evaluation
evaluate_model_weighted_optimized(icat_processed, category_features_weighted, rats.copy()) # use a copy to avoid modifying the original rats